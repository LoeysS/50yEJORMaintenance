{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e114aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybliometrics.scopus import AbstractRetrieval\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c961cd5",
   "metadata": {},
   "source": [
    "## 1. Creation of EJOR abstract dataset ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a143df0",
   "metadata": {},
   "source": [
    "The list of DOIs is created, which is the DOI column of the data provided by EJOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d640fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EJOR = pd.read_table(\".\\DATASETS\\EJOR_Breakdown.csv\", delimiter = \",\") #EJOR breakdown csv is without abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOIlist = list(EJOR['DOI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdcb9db",
   "metadata": {},
   "source": [
    "A dictionary is created with as key the DOI item and as value the abstract, retrieved by the pybliometrics library. If there is no DOI for an entry, we put NO ABSTRACT FOUND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c63a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOIdict = dict()\n",
    "for doi_index in range(1,len(DOIlist)):\n",
    "    if str(DOIlist[doi_index]) == 'nan':\n",
    "        DOIdict[DOIlist[doi_index]] = 'NO ABSTRACT FOUND'\n",
    "    else:\n",
    "        capture = AbstractRetrieval(identifier = DOIlist[doi_index], id_type = \"doi\")\n",
    "        DOIdict[DOIlist[doi_index]] = capture.description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b2fd5",
   "metadata": {},
   "source": [
    "Write it to a txt file, for safety. The append ('a') is handy such that when the program gets stuck you can add the parts that have already been retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162816e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"all_abstracts.txt.txt\", 'a', encoding = \"utf-8\")\n",
    "for key in DOIdict:\n",
    "    f.write(str(key) + \"\\t\" + str(DOIdict[key]) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fe4b4",
   "metadata": {},
   "source": [
    "In the next part, we read the table of all DOIs with their abstract, and merge it with the table provided by EJOR. Some papers do not have a DOI. These are manually checked by going to their site on scopus, if the abstract there is relevant, we add it. Conference proceedings are not added and thus removed from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_abstracts = pd.read_table(\"all_abstracts.txt\", delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e60b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = EJOR.merge(scrape, on='DOI', how='left', indicator= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1139435",
   "metadata": {},
   "source": [
    "indices of entries without a DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a38a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lijstje = [16580, 18801] #list of values where the abscent abstract is relevant, so we do not remove them.\n",
    "for index in df[df[\"DOI\"].isna()].index:\n",
    "    if index not in lijstje:\n",
    "        df.drop(index,axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c91fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[16580,\"Abstract\"] = \"In the first part, the preparation of aqueous anionic urethane-urea dispersions is described using different aliphatic and cycloaliphatic diisocyanates, a polyether polyol (PTMG 2000), dimethylolpropionic acid (DMPA), and cyclohexane-diamine as chain extender. In the second part, different polyester and polyether polyols and different neutralizing agents were employed using1, 12-dodecane diisocyanate (C12DDI) as the sole diisocyanate and cyclohexanediamine as chain extender in the preparation of the anionic urethane-urea dispersions. The relationships between the chemical structure of the diisocyanates, polyols, and neutralizing agents on the dispersion, mechanical and thermal properties are being discussed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[18801,\"Abstract\"] = \"The quantification of the financial benefits of computerized information systems is discussed. It is relatively easy to analyze the clerical applications of computers. In management information systems, however, revenues arise only if computers yield better data and if these data are used to improve decision-making. A new framework is presented plus a few theories and techniques. The framework comprises the sequence transaction-data creation-decision-reaction. Relevant theories are Bayesian Information Economics, Control Theory, and System Dynamics. Relevant techniques are simulation and management gaming.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef7315",
   "metadata": {},
   "source": [
    "open up the keyword list with their DOI. Then we make it a dataframe and merge it with the other dataframe, based on the DOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a69a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\DATASETS\\keyworddict.txt\", encoding= \"utf-8\") as f:\n",
    "    DOIdict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c931bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dois = []\n",
    "keywordas = []\n",
    "for key in DOIdict:\n",
    "    dois.append(key)\n",
    "    if DOIdict[key] is not None:\n",
    "        keywordas.append(tuple(DOIdict[key]))\n",
    "    else:\n",
    "        keywordas.append(tuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce7bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(list(zip(dois,keywordas)),columns=['DOI','Keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.merge(df2, left_on = 'DOI', right_on = 'DOI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff35c8b",
   "metadata": {},
   "source": [
    "write away to a csv file for a temporary save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dea84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\".\\DATASETS\\EJOR_DATABASE_ABSTRACT_KEYWORDS.csv\", index = False, encoding = 'utf-8', delimiter = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2fe05",
   "metadata": {},
   "source": [
    "The next section will clean up the text of title, keywords provided by authors, and abstract. Then all that text is concatenated for later basic text-mining use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".\\DATASETS\\EJOR_DATABASE_ABSTRACT_KEYWORDS.csv\") #read in data with year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc96360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Title'] + ' ' + df['Abstract'] + ' ' + df['Keywords']\n",
    "#merge all text data in one column and then clean it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b54916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting ready for text mining\n",
    "#import nltk\n",
    "#nltk.download('stopwords') #do this if running code first time\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "#Clean Text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    text = ' '.join(text.split()) \n",
    "    return text\n",
    "\n",
    "#stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c97247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: remove_stopwords(x))\n",
    "df['Text'] = df['Text'].apply(lambda x:clean_text(x))\n",
    "df['Text'] = df['Text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75642289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\".\\DATASETS\\EJOR_DATABASE_ABSTRACT_KEYWORDS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual analysis are done in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
